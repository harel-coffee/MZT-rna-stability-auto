{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curves for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import learning_curve, GroupKFold\n",
    "\n",
    "from optimalcodon.projects.rnastability.dataprocessing import get_data, general_preprocesing_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67775 points for training and 7576 for testing with 6 features\n"
     ]
    }
   ],
   "source": [
    "# 1 DATA PRE-PROCESSING\n",
    "\n",
    "(train_x, train_y), (test_x, test_y) = get_data(\n",
    "    '../data/191004-TrainAndTestSets/')\n",
    "print(\"{} points for training and {} for testing with {} features\".format(\n",
    "    train_x.shape[0], test_x.shape[0], test_x.shape[1]))\n",
    "\n",
    "groups = train_x.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General cross-validation strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function\n",
    "def MYlearning_curve(mdl_id_name, train_x, train_y, estimator):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        mdl_id_name (str): id to identify model\n",
    "        train_x: training predictors, should be pre-processed for the\n",
    "            particular model to be evaluated\n",
    "        train_y: training labels\n",
    "        estimator: model\n",
    "        cv: grouped k-fold\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    cv = GroupKFold(n_splits=5).split(train_x, train_y, groups=groups)\n",
    "    \n",
    "    print('generating learning curve for ' + mdl_id_name)\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator=estimator,\n",
    "        cv=cv,\n",
    "        X=train_x,\n",
    "        y=train_y,\n",
    "        train_sizes=np.linspace(0.1, 1, 5),\n",
    "        scoring='r2',\n",
    "        n_jobs=25,\n",
    "        verbose=10\n",
    "    )\n",
    "    \n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    learning_curve_results = pd.DataFrame({\n",
    "        'train_sizes': train_sizes,\n",
    "        'train_scores_mean': train_scores_mean,\n",
    "        'train_scores_std': train_scores_std,\n",
    "        'test_scores_mean': test_scores_mean,\n",
    "        'test_scores_std': test_scores_std\n",
    "    })\n",
    "    \n",
    "    learning_curve_results['model'] = mdl_id_name\n",
    "    return learning_curve_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Lasso, ElasticNet\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "linear_models = dict(\n",
    "    lasso=\"../191004-TrainPredictiveModelsMrnaStability/results_data/trained_models/lasso.joblib\",\n",
    "    enet=\"../191004-TrainPredictiveModelsMrnaStability/results_data/trained_models/enet.joblib\",\n",
    "    linear=\"../191004-TrainPredictiveModelsMrnaStability/results_data/trained_models/linear_reg.joblib\",\n",
    "    pls =\"../191004-TrainPredictiveModelsMrnaStability/results_data/trained_models/PLS.joblib\"\n",
    ")\n",
    "\n",
    "# load the models\n",
    "linear_models = {x:joblib.load(linear_models[x]) for x in linear_models}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this linear model I use a specific preprocessing pipeline to add 2nd degree polynomial. Next, I define this preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing\n",
    "\n",
    "preprocessing = Pipeline([\n",
    "    ('general', general_preprocesing_pipeline(train_x)), # see the code for general_preprocesing_pipeline\n",
    "    ('polyfeaturs', PolynomialFeatures(degree=2)),\n",
    "    ('zerovar', VarianceThreshold(threshold=0.0)),\n",
    "    ('scaling', StandardScaler()) # I scale again not all polynomial features may be with scaled\n",
    "])\n",
    "\n",
    "\n",
    "preprocessing.fit(train_x)\n",
    "train_x_transformed_for_linear = preprocessing.transform(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating learning curve for lasso\n",
      "[learning_curve] Training set sizes: [ 5422 17621 29821 42020 54220]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   2 out of  25 | elapsed:   41.1s remaining:  7.9min\n",
      "[Parallel(n_jobs=25)]: Done   5 out of  25 | elapsed:   43.8s remaining:  2.9min\n",
      "[Parallel(n_jobs=25)]: Done   8 out of  25 | elapsed:   50.8s remaining:  1.8min\n",
      "[Parallel(n_jobs=25)]: Done  11 out of  25 | elapsed:   56.2s remaining:  1.2min\n",
      "[Parallel(n_jobs=25)]: Done  14 out of  25 | elapsed:  1.1min remaining:   50.3s\n",
      "[Parallel(n_jobs=25)]: Done  17 out of  25 | elapsed:  1.1min remaining:   30.4s\n",
      "[Parallel(n_jobs=25)]: Done  20 out of  25 | elapsed:  1.1min remaining:   16.6s\n",
      "[Parallel(n_jobs=25)]: Done  23 out of  25 | elapsed:  1.2min remaining:    6.3s\n",
      "[Parallel(n_jobs=25)]: Done  25 out of  25 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating learning curve for enet\n",
      "[learning_curve] Training set sizes: [ 5422 17621 29821 42020 54220]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   2 out of  25 | elapsed:   40.7s remaining:  7.8min\n",
      "[Parallel(n_jobs=25)]: Done   5 out of  25 | elapsed:   46.5s remaining:  3.1min\n",
      "[Parallel(n_jobs=25)]: Done   8 out of  25 | elapsed:  1.3min remaining:  2.8min\n",
      "[Parallel(n_jobs=25)]: Done  11 out of  25 | elapsed:  1.8min remaining:  2.3min\n",
      "[Parallel(n_jobs=25)]: Done  14 out of  25 | elapsed:  2.0min remaining:  1.5min\n",
      "[Parallel(n_jobs=25)]: Done  17 out of  25 | elapsed:  2.5min remaining:  1.2min\n",
      "[Parallel(n_jobs=25)]: Done  20 out of  25 | elapsed:  2.8min remaining:   41.5s\n",
      "[Parallel(n_jobs=25)]: Done  23 out of  25 | elapsed:  3.1min remaining:   16.2s\n",
      "[Parallel(n_jobs=25)]: Done  25 out of  25 | elapsed:  3.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating learning curve for linear\n",
      "[learning_curve] Training set sizes: [ 5422 17621 29821 42020 54220]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   2 out of  25 | elapsed:  1.1min remaining: 13.0min\n",
      "[Parallel(n_jobs=25)]: Done   5 out of  25 | elapsed:  1.3min remaining:  5.3min\n",
      "[Parallel(n_jobs=25)]: Done   8 out of  25 | elapsed:  1.6min remaining:  3.5min\n",
      "[Parallel(n_jobs=25)]: Done  11 out of  25 | elapsed:  2.0min remaining:  2.5min\n",
      "[Parallel(n_jobs=25)]: Done  14 out of  25 | elapsed:  2.3min remaining:  1.8min\n",
      "[Parallel(n_jobs=25)]: Done  17 out of  25 | elapsed:  2.5min remaining:  1.2min\n",
      "[Parallel(n_jobs=25)]: Done  20 out of  25 | elapsed:  2.6min remaining:   39.3s\n",
      "[Parallel(n_jobs=25)]: Done  23 out of  25 | elapsed:  3.0min remaining:   15.6s\n",
      "[Parallel(n_jobs=25)]: Done  25 out of  25 | elapsed:  3.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating learning curve for pls\n",
      "[learning_curve] Training set sizes: [ 5422 17621 29821 42020 54220]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   2 out of  25 | elapsed:   26.7s remaining:  5.1min\n",
      "[Parallel(n_jobs=25)]: Done   5 out of  25 | elapsed:   28.6s remaining:  1.9min\n",
      "[Parallel(n_jobs=25)]: Done   8 out of  25 | elapsed:   35.5s remaining:  1.3min\n",
      "[Parallel(n_jobs=25)]: Done  11 out of  25 | elapsed:   40.7s remaining:   51.8s\n",
      "[Parallel(n_jobs=25)]: Done  14 out of  25 | elapsed:   51.2s remaining:   40.2s\n",
      "[Parallel(n_jobs=25)]: Done  17 out of  25 | elapsed:   52.0s remaining:   24.5s\n",
      "[Parallel(n_jobs=25)]: Done  20 out of  25 | elapsed:   54.4s remaining:   13.6s\n",
      "[Parallel(n_jobs=25)]: Done  23 out of  25 | elapsed:   56.5s remaining:    4.9s\n",
      "[Parallel(n_jobs=25)]: Done  25 out of  25 | elapsed:  1.3min finished\n"
     ]
    }
   ],
   "source": [
    "res_linear = []\n",
    "\n",
    "for mdl_id, estimator in linear_models.items():\n",
    "    tmp_res = MYlearning_curve(mdl_id, train_x_transformed_for_linear, train_y, estimator)\n",
    "    res_linear.append(tmp_res)\n",
    "\n",
    "res_linear = pd.concat(res_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_linear['type'] = \"linear\"\n",
    "res_linear.to_csv('results-data/lc-linear.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Non-linear and Tree models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_linear_models = dict(\n",
    "    knn=\"../191004-TrainPredictiveModelsMrnaStability/results_data/trained_models/knn.joblib\",\n",
    "    adaBoost=\"../191004-TrainPredictiveModelsMrnaStability/results_data/trained_models/AdaBoost.joblib\",\n",
    "    decisionTree=\"../191004-TrainPredictiveModelsMrnaStability/results_data/trained_models/decision tree.joblib\",\n",
    "    gbm=\"../191004-TrainPredictiveModelsMrnaStability/results_data/trained_models/gbm.joblib\",\n",
    "    randomforest=\"../191004-TrainPredictiveModelsMrnaStability/results_data/trained_models/random forest.joblib\"\n",
    "    \n",
    ")\n",
    "# load the models\n",
    "non_linear_models = {x:joblib.load(non_linear_models[x]) for x in non_linear_models}\n",
    "\n",
    "# reset the params for random forest\n",
    "non_linear_models['randomforest'] = non_linear_models['randomforest'].set_params(n_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next model we use the general pre-processing pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_pipeline = general_preprocesing_pipeline(train_x)\n",
    "train_x_transformed = general_pipeline.transform(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating learning curve for knn\n",
      "[learning_curve] Training set sizes: [ 5422 17621 29821 42020 54220]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   2 out of  25 | elapsed:   49.4s remaining:  9.5min\n",
      "[Parallel(n_jobs=25)]: Done   5 out of  25 | elapsed:   55.3s remaining:  3.7min\n",
      "[Parallel(n_jobs=25)]: Done   8 out of  25 | elapsed:  4.3min remaining:  9.0min\n",
      "[Parallel(n_jobs=25)]: Done  11 out of  25 | elapsed:  8.3min remaining: 10.6min\n",
      "[Parallel(n_jobs=25)]: Done  14 out of  25 | elapsed:  9.7min remaining:  7.6min\n",
      "[Parallel(n_jobs=25)]: Done  17 out of  25 | elapsed: 16.0min remaining:  7.5min\n",
      "[Parallel(n_jobs=25)]: Done  20 out of  25 | elapsed: 18.3min remaining:  4.6min\n",
      "[Parallel(n_jobs=25)]: Done  23 out of  25 | elapsed: 24.0min remaining:  2.1min\n",
      "[Parallel(n_jobs=25)]: Done  25 out of  25 | elapsed: 26.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating learning curve for adaBoost\n",
      "[learning_curve] Training set sizes: [ 5422 17621 29821 42020 54220]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   2 out of  25 | elapsed:   46.0s remaining:  8.8min\n",
      "[Parallel(n_jobs=25)]: Done   5 out of  25 | elapsed:   46.7s remaining:  3.1min\n",
      "[Parallel(n_jobs=25)]: Done   8 out of  25 | elapsed:  2.3min remaining:  4.9min\n",
      "[Parallel(n_jobs=25)]: Done  11 out of  25 | elapsed:  3.7min remaining:  4.7min\n",
      "[Parallel(n_jobs=25)]: Done  14 out of  25 | elapsed:  4.0min remaining:  3.1min\n",
      "[Parallel(n_jobs=25)]: Done  17 out of  25 | elapsed:  5.3min remaining:  2.5min\n",
      "[Parallel(n_jobs=25)]: Done  20 out of  25 | elapsed:  5.6min remaining:  1.4min\n",
      "[Parallel(n_jobs=25)]: Done  23 out of  25 | elapsed:  6.7min remaining:   35.1s\n",
      "[Parallel(n_jobs=25)]: Done  25 out of  25 | elapsed:  6.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating learning curve for decisionTree\n",
      "[learning_curve] Training set sizes: [ 5422 17621 29821 42020 54220]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   2 out of  25 | elapsed:    1.4s remaining:   15.7s\n",
      "[Parallel(n_jobs=25)]: Done   5 out of  25 | elapsed:    2.2s remaining:    8.7s\n",
      "[Parallel(n_jobs=25)]: Done   8 out of  25 | elapsed:    2.5s remaining:    5.4s\n",
      "[Parallel(n_jobs=25)]: Done  11 out of  25 | elapsed:    2.9s remaining:    3.7s\n",
      "[Parallel(n_jobs=25)]: Done  14 out of  25 | elapsed:    3.6s remaining:    2.8s\n",
      "[Parallel(n_jobs=25)]: Done  17 out of  25 | elapsed:    4.7s remaining:    2.2s\n",
      "[Parallel(n_jobs=25)]: Done  20 out of  25 | elapsed:    5.2s remaining:    1.3s\n",
      "[Parallel(n_jobs=25)]: Done  23 out of  25 | elapsed:    7.3s remaining:    0.6s\n",
      "[Parallel(n_jobs=25)]: Done  25 out of  25 | elapsed:    8.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating learning curve for gbm\n",
      "[learning_curve] Training set sizes: [ 5422 17621 29821 42020 54220]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   2 out of  25 | elapsed:  3.8min remaining: 43.5min\n",
      "[Parallel(n_jobs=25)]: Done   5 out of  25 | elapsed:  4.0min remaining: 15.8min\n",
      "[Parallel(n_jobs=25)]: Done   8 out of  25 | elapsed:  8.3min remaining: 17.6min\n",
      "[Parallel(n_jobs=25)]: Done  11 out of  25 | elapsed: 12.3min remaining: 15.6min\n",
      "[Parallel(n_jobs=25)]: Done  14 out of  25 | elapsed: 12.8min remaining: 10.0min\n",
      "[Parallel(n_jobs=25)]: Done  17 out of  25 | elapsed: 19.2min remaining:  9.0min\n",
      "[Parallel(n_jobs=25)]: Done  20 out of  25 | elapsed: 20.8min remaining:  5.2min\n",
      "[Parallel(n_jobs=25)]: Done  23 out of  25 | elapsed: 25.6min remaining:  2.2min\n",
      "[Parallel(n_jobs=25)]: Done  25 out of  25 | elapsed: 27.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating learning curve for randomforest\n",
      "[learning_curve] Training set sizes: [ 5422 17621 29821 42020 54220]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "/home/smedina/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=25)]: Done   2 out of  25 | elapsed:  5.7min remaining: 65.3min\n",
      "[Parallel(n_jobs=25)]: Done   5 out of  25 | elapsed:  5.9min remaining: 23.5min\n",
      "[Parallel(n_jobs=25)]: Done   8 out of  25 | elapsed: 19.7min remaining: 41.8min\n",
      "[Parallel(n_jobs=25)]: Done  11 out of  25 | elapsed: 32.1min remaining: 40.9min\n",
      "[Parallel(n_jobs=25)]: Done  14 out of  25 | elapsed: 34.8min remaining: 27.3min\n",
      "[Parallel(n_jobs=25)]: Done  17 out of  25 | elapsed: 46.6min remaining: 21.9min\n",
      "[Parallel(n_jobs=25)]: Done  20 out of  25 | elapsed: 49.0min remaining: 12.3min\n",
      "[Parallel(n_jobs=25)]: Done  23 out of  25 | elapsed: 59.4min remaining:  5.2min\n",
      "[Parallel(n_jobs=25)]: Done  25 out of  25 | elapsed: 60.1min finished\n"
     ]
    }
   ],
   "source": [
    "res_nonlinear = []\n",
    "\n",
    "for mdl_id, estimator in non_linear_models.items():\n",
    "    tmp_res = MYlearning_curve(mdl_id, train_x_transformed, train_y, estimator)\n",
    "    res_nonlinear.append(tmp_res)\n",
    "\n",
    "res_nonlinear = pd.concat(res_nonlinear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_nonlinear['type'] = \"non-linear\"\n",
    "res_nonlinear.to_csv('results-data/lc-nonlinear.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
