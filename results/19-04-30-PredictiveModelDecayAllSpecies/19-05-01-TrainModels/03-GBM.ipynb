{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## GBM tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import random_integers\n",
    "import pandas as pd\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "\n",
    "# my module imports\n",
    "from optimalcodon.projects.rnastability.dataprocessing import get_data, general_preprocesing_pipeline\n",
    "from optimalcodon.projects.rnastability import modelevaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67817 points for training and 7534 for testing with 6 features\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(67817, 80)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_x, train_y), (test_x, test_y) = get_data(\"../19-04-30-EDA/results_data/\")\n",
    "print(\"{} points for training and {} for testing with {} features\".format(\n",
    "    train_x.shape[0], test_x.shape[0], test_x.shape[1]))\n",
    "\n",
    "# pre-processing\n",
    "\n",
    "preprocessing = general_preprocesing_pipeline(train_x)\n",
    "\n",
    "preprocessing.fit(train_x)\n",
    "train_x_transformed = preprocessing.transform(train_x)\n",
    "\n",
    "train_x_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 70 candidates, totalling 350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done   1 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=30)]: Done  12 tasks      | elapsed: 68.1min\n",
      "/home/smedina/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=30)]: Done  25 tasks      | elapsed: 101.2min\n",
      "[Parallel(n_jobs=30)]: Done  38 tasks      | elapsed: 246.2min\n",
      "[Parallel(n_jobs=30)]: Done  53 tasks      | elapsed: 342.2min\n",
      "[Parallel(n_jobs=30)]: Done  68 tasks      | elapsed: 410.4min\n",
      "[Parallel(n_jobs=30)]: Done  85 tasks      | elapsed: 451.2min\n",
      "[Parallel(n_jobs=30)]: Done 102 tasks      | elapsed: 482.9min\n",
      "[Parallel(n_jobs=30)]: Done 121 tasks      | elapsed: 533.8min\n",
      "[Parallel(n_jobs=30)]: Done 140 tasks      | elapsed: 597.4min\n",
      "[Parallel(n_jobs=30)]: Done 161 tasks      | elapsed: 684.5min\n",
      "[Parallel(n_jobs=30)]: Done 182 tasks      | elapsed: 755.6min\n",
      "[Parallel(n_jobs=30)]: Done 205 tasks      | elapsed: 805.5min\n",
      "[Parallel(n_jobs=30)]: Done 228 tasks      | elapsed: 862.9min\n",
      "[Parallel(n_jobs=30)]: Done 253 tasks      | elapsed: 980.7min\n",
      "[Parallel(n_jobs=30)]: Done 278 tasks      | elapsed: 1061.5min\n",
      "[Parallel(n_jobs=30)]: Done 327 out of 350 | elapsed: 1275.5min remaining: 89.7min\n",
      "[Parallel(n_jobs=30)]: Done 350 out of 350 | elapsed: 1628.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=KFold(n_splits=5, random_state=42, shuffle=True),\n",
       "          error_score='raise-deprecating',\n",
       "          estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_sampl...=None, subsample=1.0, tol=0.0001,\n",
       "             validation_fraction=0.1, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid='warn', n_iter=70, n_jobs=30,\n",
       "          param_distributions={'loss': ['huber'], 'learning_rate': [0.1, 0.055, 0.01, 0.0055, 0.001], 'n_estimators': [500, 700, 1000, 1200, 1500, 1700, 2000], 'min_samples_split': [4, 6, 8], 'min_samples_leaf': [2, 3], 'max_depth': [10, 15, 17, 20], 'max_features': ['log2']},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='r2', verbose=10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm_grid = {'loss':['huber'],\n",
    "          'learning_rate':[0.1, 0.055, 0.01, 0.0055, 0.001],\n",
    "          'n_estimators':[500, 700,1000, 1200, 1500, 1700, 2000],\n",
    "          'min_samples_split':[4, 6, 8],\n",
    "          'min_samples_leaf':[2, 3],\n",
    "          'max_depth':[10, 15, 17, 20],\n",
    "          'max_features':['log2']}\n",
    "\n",
    "# make cross validation to shuffle the data\n",
    "cross_val = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "model = GradientBoostingRegressor()\n",
    "random_cv = RandomizedSearchCV(estimator=model,\n",
    "                               param_distributions=gbm_grid,\n",
    "                               cv=cross_val, n_iter=70, n_jobs=30,verbose=10,\n",
    "                               scoring='r2')\n",
    "\n",
    "random_cv.fit(train_x_transformed, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3700031529927131"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 2000,\n",
       " 'min_samples_split': 8,\n",
       " 'min_samples_leaf': 3,\n",
       " 'max_features': 'log2',\n",
       " 'max_depth': 10,\n",
       " 'loss': 'huber',\n",
       " 'learning_rate': 0.01}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.01, loss='huber', max_depth=10,\n",
       "             max_features='log2', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=3, min_samples_split=8,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=2000,\n",
       "             n_iter_no_change=None, presort='auto', random_state=None,\n",
       "             subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating predictions for model: gbm\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'gbm': random_cv.best_estimator_\n",
    "}\n",
    "modelevaluation.eval_models(models, preprocessing, test_x, test_y).to_csv(\"results_data/val_gbm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv for model: gbm\n"
     ]
    }
   ],
   "source": [
    "modelevaluation.crossvalidation(models, train_x_transformed, train_y).to_csv('results_data/cv_gbm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smedina/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/smedina/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/smedina/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/smedina/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/smedina/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/smedina/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/smedina/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(random_cv.cv_results_).to_csv(\"results_data/tuning_params_gbm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
