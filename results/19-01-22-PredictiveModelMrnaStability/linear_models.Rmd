---
title: "Linear Models"
author: "Santiago Medina"
date: "1/22/2019"
output: 
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = F, message = F, warning = F, collapse = T,
  fig.path = "./figures/linear_", dev = c("png", "pdf"),
  dpi=300,
  fig.height = 3,
  fig.width = 5
  )

# libraries need it for analysis
library(tidyverse)
library(gridExtra)
library(recipes)
library(caret)
library(doParallel)
cl <- makePSOCKcluster(4)
registerDoParallel(cl)

theme_set(theme_bw())


decay <- codonr::load_decay_aa_codon_composition_data(
  cc_dp = "../../data/19-01-17-Get-ORFS-UTRS-codon-composition/sequence-data/zfish_codon_composition.csv",
  decay_dp = "../19-01-11-GetDecayRateFromTimeCourse/results_data/estimated_decay_rates.csv"
)

```



### Overview

In this notebook I will evaluate the folling linear models:

+ Linear Regression
+ Partial Least Squares
+ Ridge Regression
+ Lasso Regression
+ Elastic Net


First, I define a common recipe to preprocces the data.

```{r preprocessing}
set.seed(10)
dsplit <- rsample::initial_split(decay)
training <- rsample::analysis(dsplit)
test <- rsample::assessment(dsplit)

codon_recipe <- 
  recipe(decay_rate ~ ., data = training)  %>%
  update_role(Gene_ID, new_role = "id variable") %>%
  step_spatialsign(all_predictors()) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors())
```

```{r training}

controlObject <- trainControl(method = "repeatedcv", repeats = 3, number = 10)

lm_fit <- train(
  codon_recipe,
  data = training,
  method = "lm",
  trControl = controlObject
)

pls_fit <- train(
  codon_recipe,
  data = training,
  method = "pls",
  tuneLength = 61,
  trControl = controlObject
)

ridge_grid <- data.frame(.lambda = seq(0, 1, length.out = 10))
ridge_fit <- train(
  codon_recipe,
  data = training,
  method = "ridge",
  tuneGrid = ridge_grid,
  trControl = controlObject
)

enet_grid <- expand.grid(lambda = c(0, 0.01, .1), fraction = seq(.01, 1, length.out = 30))
enet_fit <- train(
  codon_recipe,
  data = training,
  method = "enet",
  tuneGrid = enet_grid,
  trControl = controlObject
)
```

Visualize tuning results.

```{r data_viz_models}
pls_fit$results %>% 
  ggplot(aes(ncomp, Rsquared)) +
  geom_line() +
  geom_point(shape=1) +
  geom_linerange(
    aes(ymax  = Rsquared + 1.96*RsquaredSD, ymin = Rsquared - 1.96*RsquaredSD),
    alpha = 1/2
    ) +
  coord_cartesian(ylim = c(0, .3)) +
  labs(title = "PLS model", subtitle = str_c("best tune: ", pls_fit$bestTune))

ridge_fit$results %>% 
  ggplot(aes(lambda, Rsquared)) +
  geom_point() +
  geom_line()
  
enet_fit$results %>% 
  ggplot(aes(fraction, Rsquared, color = lambda)) +
  geom_point() +
  geom_line() +
  labs(title = "enet")

```


```{r}
all_resamples <- resamples(list(
  "liner reg" = lm_fit,
  "pls" = pls_fit,
  "ridge" = ridge_fit,
  "ene" = enet_fit
))

parallelplot(all_resamples, metric = "Rsquared")
```

```{r results_linear_models}
as.matrix(all_resamples, metric = "Rsquared") %>% 
  as_tibble(rownames = "resample") %>% 
  gather(key = mdl, value = Rsquare, -resample) %>% 
  ggplot(aes(mdl, Rsquare)) +
  ggthemes::geom_tufteboxplot() +
  coord_flip()
```


```{r residual_predicted, fig.width=7.5, fig.height=3}

val <- tibble(
  y = test$decay_rate,
  lm = predict(enet_fit, newdata = test),
  ridge = predict(ridge_fit, newdata = test),
  pls = predict(pls_fit, newdata = test),
  enet =predict(pls_fit, newdata = test)
) %>% 
  gather(key = mdl, value = prediction, -y) %>% 
  mutate(resid = y - prediction)

p1 <- 
  val %>% 
  ggplot(aes(x = prediction, y = y)) +
  geom_point(size = 1/4, alpha=1/2) +
  geom_abline() +
  ggpubr::stat_cor(size = 2) +
  facet_grid(~mdl)

p2 <- 
  val %>% 
  ggplot(aes(x = prediction, y = resid)) +
  geom_point(size = 1/4, alpha = 1/2) +
  ggpubr::stat_cor(size = 2) +
  facet_grid(~mdl)

grid.arrange(p1, p2, nrow = 2)
```


**Conclusion**

The performance of these models is about 20% (R-squared). If I have to pick one, I will choose the PLS with the 3 components since these components can be used for the other modeling purposes (defining new elements).
